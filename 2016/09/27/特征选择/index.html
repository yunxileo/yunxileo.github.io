<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Feature Selection | 云稀里</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1.特征选择的概述
1.1  特征工程的简介相信每一个数据人都深刻地体会过这样一句话：“特征工程做不好，调参调到老”。业界更有这样的说法：“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。由此可见，特征工程在机器学习中占有相当重要的地位。到底什么是特征工程，我认为并没有确切的定义，个人是这样理解的：特征工程是利用业务领域和数据科学领域的相关知识，创建使机器学习算法达到最优的特">
<meta property="og:type" content="article">
<meta property="og:title" content="Feature Selection">
<meta property="og:url" content="http://yunxileo.github.io/2016/09/27/特征选择/index.html">
<meta property="og:site_name" content="云稀里">
<meta property="og:description" content="1.特征选择的概述
1.1  特征工程的简介相信每一个数据人都深刻地体会过这样一句话：“特征工程做不好，调参调到老”。业界更有这样的说法：“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。由此可见，特征工程在机器学习中占有相当重要的地位。到底什么是特征工程，我认为并没有确切的定义，个人是这样理解的：特征工程是利用业务领域和数据科学领域的相关知识，创建使机器学习算法达到最优的特">
<meta property="og:image" content="http://yunxileo.github.io/img/process.png">
<meta property="og:image" content="http://yunxileo.github.io/img/iv.png">
<meta property="og:image" content="http://yunxileo.github.io/img/bar.png">
<meta property="og:image" content="http://yunxileo.github.io/img/rf.png">
<meta property="og:image" content="http://yunxileo.github.io/img/rfe.png">
<meta property="og:updated_time" content="2017-02-06T02:54:49.764Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Feature Selection">
<meta name="twitter:description" content="1.特征选择的概述
1.1  特征工程的简介相信每一个数据人都深刻地体会过这样一句话：“特征工程做不好，调参调到老”。业界更有这样的说法：“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。由此可见，特征工程在机器学习中占有相当重要的地位。到底什么是特征工程，我认为并没有确切的定义，个人是这样理解的：特征工程是利用业务领域和数据科学领域的相关知识，创建使机器学习算法达到最优的特">
<meta name="twitter:image" content="http://yunxileo.github.io/img/process.png">
  
    <link rel="alternative" href="/atom.xml" title="云稀里" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
      <link href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css" rel="stylesheet">
  
  
  
      <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
      <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">
  
  <link rel="stylesheet" href="/css/style.css">
  
  <link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">
  
  <script src="//cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/clipboard.js/1.5.9/clipboard.min.js"></script>
  <script>
      var yiliaConfig = {
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false,
          fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
          scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.0.9/scrollreveal.min.js"
      }
  </script>

  
      <script>
          yiliaConfig.rootUrl = "/";
      </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

  
  
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--[if lt IE 9]>
<style> body { background: orange !important;} div#container { display: none;} </style>
<p id="ie-updater">
    
        珍爱生命，远离 IE！<br>本站不支持 IE6/7/8，请升级浏览器！<br>
    
    <a href="//www.bing.com/search?q=Chrome" target="_blank">Chrome</a>, <a href="//www.bing.com/search?q=Firefox" target="_blank">Firefox</a><br>
    Hexo Theme <a href="//github.com/MOxFIVE/hexo-theme-yelee" target="_blank">Yelee</a> by MOxFIVE
</p>
<![endif]-->
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">yunxileo</a></h1>
        </hgroup>

        
        <p class="header-subtitle">莫厌追欢笑语频，寻思离乱好伤神。闲来屈指从头数，得见清平有几人</p>
        
                


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:yunxileo@gmail.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/R语言编程/">-R语言编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/特征选择/">-特征选择</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/缺失值处理/">缺失值处理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/逻辑回归/">逻辑回归</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">yunxileo</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">yunxileo</a></h1>
            </hgroup>
            
            <p class="header-subtitle">莫厌追欢笑语频，寻思离乱好伤神。闲来屈指从头数，得见清平有几人</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:yunxileo@gmail.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-特征选择" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/27/特征选择/" class="article-date">
      <time datetime="2016-09-27T07:35:15.000Z" itemprop="datePublished">2016-09-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Feature Selection
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a><a class="article-category-link" href="/categories/机器学习/特征选择/">特征选择</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/特征选择/">-特征选择</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <blockquote>
<h2 id="1-特征选择的概述"><a href="#1-特征选择的概述" class="headerlink" title="1.特征选择的概述"></a>1.特征选择的概述</h2></blockquote>
<h3 id="1-1-特征工程的简介"><a href="#1-1-特征工程的简介" class="headerlink" title="1.1  特征工程的简介"></a>1.1  特征工程的简介</h3><p>相信每一个数据人都深刻地体会过这样一句话：“特征工程做不好，调参调到老”。业界更有这样的说法：“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。由此可见，特征工程在机器学习中占有相当重要的地位。到底什么是特征工程，我认为并没有确切的定义，个人是这样理解的：特征工程是利用业务领域和数据科学领域的相关知识，创建使机器学习算法达到最优的特征的过程。特征工程主要包括特征抽取和特征选择两部分。特征抽取主要是依据相关领域知识提取特征，更多的依靠业务知识。下文将重点介绍特征选择。</p>
<h3 id="1-2-特征选择的简介"><a href="#1-2-特征选择的简介" class="headerlink" title="1.2 特征选择的简介"></a>1.2 特征选择的简介</h3><p>(1) 什么是特征选择</p>
<p>特征选择 ( Feature Selection )也称特征子集选择( Feature Subset Selection , FSS ) ，或属性选择( Attribute Selection ) ，是指从全部特征中选取一个特征子集，使构造出来的模型更好。</p>
<p>(2) 为什么要做特征选择</p>
<p>在机器学习的实际应用中，特征数量往往较多，其中可能存在不相关的特征，特征之间也可能存在相互依赖，容易导致如下的后果：</p>
<ul>
<li>特征个数越多，分析特征、训练模型所需的时间就越长。</li>
</ul>
<ul>
<li>特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。</li>
</ul>
<ul>
<li>特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。</li>
</ul>
<ul>
<li>选取出真正相关的特征简化了模型，使研究人员易于理解数据产生的过程。</li>
</ul>
<h3 id="1-3-特征选择的过程"><a href="#1-3-特征选择的过程" class="headerlink" title="1.3 特征选择的过程"></a>1.3 特征选择的过程</h3><p>Dash和Liu提出了一个特征选择的基本框架，认为一个典型的特征选择<br>算法由图所示的4个基本步骤组成</p>
<p><img src="/img/process.png" alt="特征选择的过程"></p>
<p>(1)子集产生：按照一定的搜索策略产生候选特征子集。</p>
<p>(2)子集评估：通过某个评价函数评估特征子集的优劣。</p>
<p>(3)停止条件：决定特征选择算法什么时候停止。</p>
<p>(4)子集验证：用于验证最终所选的特征子集的有效性。</p>
<p>这四个步骤组成了一个完整的特征选择过程。</p>
<ul>
<li>子集产生是一个搜索过程， 用来产生待评价的特征子集。搜索过程的起始点可以是：不含任何特征的空集；含所有特征的全部特征集；随机的一个特征子集。前面两种情况，特征是迭代地添加或移除。</li>
</ul>
<ul>
<li>子集评估过程用于评价特征子集的优劣，即根据某个评价函数对子集进行评价，每进行一次特征子集的优劣评价，将新的评价值和之前保存的最优评价值进行比较，如果新的子集的评价值更优，则用它来取代之前的最优评价值。</li>
</ul>
<ul>
<li>子集评估之后要进行停止条件的判断，如果没有停止条件，搜索过程将无法停止。如图所示，子集产生和子集评估都可能影响到停止条件的选择，基于子集产生的停止条件有：a．达到预先指定的特征数目。b．达到预先给定的迭代次数。基于子集评估的停止条件有：a．增加或减少特征不能使特征子集的评价值有所提高。b．使评价函数取最优解的特征子集已经找到。</li>
</ul>
<ul>
<li>子集有效性验证是特征选择的最后一个步骤，在实际应用中是必不可少的，有效性验证通常用经过特征选择算法处理后的数据集进行训练和预测，将训练和预测的结果和在原始数据集上的结果进行比较，比较的内容包括预测的准确性、模型的复杂程度等。</li>
</ul>
<blockquote>
<h2 id="2-特征选择常用的方法"><a href="#2-特征选择常用的方法" class="headerlink" title="2.特征选择常用的方法"></a>2.特征选择常用的方法</h2></blockquote>
<ul>
<li>按照特征子集的形成方式可以分为三种：完全搜索(Complete)、启发式搜索（heuristic）和随机搜索。</li>
</ul>
<ul>
<li>按照特征评价标准来分，根据评价函数与模型的关系，可以分为筛选式、封装式和嵌入式三种。</li>
</ul>
<h3 id="2-1-基于特征子集的形成方式的特征选择方法"><a href="#2-1-基于特征子集的形成方式的特征选择方法" class="headerlink" title="2.1 基于特征子集的形成方式的特征选择方法"></a>2.1 基于特征子集的形成方式的特征选择方法</h3><p>基于特征子集的形成方式的特征选择方法主要有完全搜索(Complete)、启发式搜索(Heuristic)和随机搜索。</p>
<p>特征选择本质上是一个组合优化问题，求解组合优化问题最直接的方法就是搜索，理论上可以通过穷举法来搜索所有可能的特征组合，选择使得评价标准最优的特征子集作为最后的输出。如果我们有N个特征变量，则特征变量的状态集合中的元素个数就是$2^N$，通过穷举的方式进行求解的时间复杂度是指数级的 (O(2^{N} ))。</p>
<p>为了减少运算量，目前特征子集的搜索策略大都采用贪心算法（greedy algorithm），其核心思想是在每一步选择中，都采纳当前条件下最好的选择，从而获得组合优化问题的近似最优解。根据其实现的细节，又可将贪心算法分为三种：前向搜索 (forward search)，后向搜索(backward search)和双向搜索 (bidirectional search)。</p>
<p>假定我们有一个特征集合${a_{1},a_{2},...,a_{d}}$,前向搜索的思想是:</p>
<ul>
<li>第一步我们可以将每一个特征看成一个候选子集，例如依据给定的评价标准，特征${a_{1}}$的效果最优，则子集为${a_{1}}$,于是将${a_{1}}$做为第一轮的选定集；</li>
</ul>
<ul>
<li>第二步，则是往已有的子集中加入下一个效果最优的特征变量，例如对于子集${a_{1},a_{i}}$，当 i=2 时效果最优，则新的子集确定为${a_{1},a_{2}}$。如此重复进行搜索，直到新一轮获得的子集效果不如前一轮，则搜索停止。</li>
</ul>
<p>后向搜索的做法，是以包含全部特征的集合开始，逐步剔除特征，直到找到效果最优的子集。双向搜索则把前向搜索和后向搜索结合起来，不断在选定的子集中加入新特征，并同时剔除旧特征。</p>
<p>因为采取的是贪心算法，它们仅考虑使本轮的子集最优，例如在第三轮假定选择$a_{5}$优于$a_{6}$,于是该轮的最优子集为${a_{1},a_{2},a_{5}}$，然后再第四轮中却可能是${a_{1},a_{2},a_{6},a_{8}}$比所有的${a_{1},a_{2},a_{5},a_{i}}$都更优。但是，若不进行穷举搜索，则这样的问题无法避免。</p>
<p>这三类算法在R语言的stats包和MASS包中的线性回归建模中都实现了。</p>
<h3 id="2-2-基于评价准测的特征选择方法"><a href="#2-2-基于评价准测的特征选择方法" class="headerlink" title="2.2 基于评价准测的特征选择方法"></a>2.2 基于评价准测的特征选择方法</h3><p>按照特征评价标准来分，根据评价函数与模型的关系，可以分为筛选式、封装式和嵌入式三种。</p>
<h4 id="2-2-1-过滤式-Filter"><a href="#2-2-1-过滤式-Filter" class="headerlink" title="2.2.1 过滤式(Filter)"></a>2.2.1 过滤式(Filter)</h4><p>通常把先进行特征选择，再进行建模的方法称为过滤式（filter），它主要侧重于单个特征跟目标变量的相关性，此时特征选择的标准和模型优化标准并不一定相同，和下一步将要使用的机器学习算法没有必然联系。</p>
<p>Dash和Liu把过滤式特征选择的评价标准分为四种，即关联度度量、距离度量、信息度量、以及一致性度量。</p>
<ul>
<li>关联性度量：<br>主要考察特征和类别的关联度以及特征间的关联度，即通常所说的相关性和冗余性。关联性度量有线性关联(如Pearson相关系数)和非线性关联(如基于信息熵的互信息、对称的不确定性等)</li>
</ul>
<ul>
<li>距离度量：运用距离度量进行特征选择是基于这样的假设：好的特征子集应该使得属于同一类的样本距离尽可能小，属于不同类的样本之间的距离尽可能远。常用的距离度量（相似性度量）包括欧氏距离、标准化欧氏距离、马氏距离以及基于概率距离度量的KL散度等。</li>
</ul>
<ul>
<li>信息度量：信息度量是把信息论中基于熵的评估标准应用得到特征选择中，如信息增益(Information Gain)、信息增益率(Information Gain Ratio)、基尼指数(Gini Index)、WOE(Weight of Evidence)以及IV(Informationvalue)等。</li>
</ul>
<ul>
<li>一致性度量：试图找到与全集相同分类能力的最小特征子集。不一致性定义为如果两个样本在选定的特征子集上取值相同，却属于不同的类。</li>
</ul>
<p>下面简单地介绍常见的评价函数：</p>
<p>(1) Pearson相关系数</p>
<p>Pearson相关系数的数学公式为：$r=\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}$</p>
<p>Pearson相关系数是最常用的判断特征和响应变量（response variable）之间的线性关系的标准。结果的取值区间为[-1，1]，-1表示完全的负相关+1表示完全的正相关，0表示没有线性相关。<br>Pearson 相关系数的优点在于其计算简单，结果易于理解且易于比较；而其缺点在于不能反映变量之间的非线性关系,如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。</p>
<p>Python中，Scipy的 pearsonr 方法能够同时计算相关系数和p-value</p>
<figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy <span class="keyword">as</span> np</span><br><span class="line">from scipy.stats import pearsonr</span><br><span class="line"><span class="keyword">x</span> = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line"><span class="keyword">print</span> pearsonr(<span class="keyword">x</span>, <span class="keyword">x</span>**<span class="number">2</span>)[<span class="number">0</span>]</span><br><span class="line">-<span class="number">0.2072</span></span><br></pre></td></tr></table></figure>
<p>在R中，用cor函数求相关系数</p>
<figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">x</span> &lt;- rnorm(<span class="number">100</span>)</span><br><span class="line">cor(<span class="keyword">x</span>,<span class="keyword">x</span>^<span class="number">2</span>)</span><br><span class="line">-<span class="number">0.132433</span></span><br></pre></td></tr></table></figure>
<p>(2)信息增益(Information Gain)</p>
<p>假设存在离散变量Y，Y中的取值包括{y1,y2,…,ym} ，yi出现的概率为Pi。则Y的信息熵定义为：<br>
$$H(Y)=-\sum_{i=1}^{m}P_{i}logP_{i}$$</p>
<p>信息熵有如下特性：若集合Y的元素分布越“纯”，则其信息熵越小；若Y分布越“混乱”，则其信息熵越大。在极端的情况下：若Y只能取一个值，即P1=1，则H(Y)取最小值0；反之若各种取值出现的概率都相等，即都是1/m，则H(Y)取最大值$log_{2}m$。</p>
<p>在附加条件另一个变量X，而且知道$X=x_{i}$后，Y的条件信息熵(Conditional Entropy)表示为：<br>
$$H(Y|X)=\sum_{i=1}^{m}P_{x=x_{i}}H(Y|X=x_{i})$$
<br>在加入条件X前后的Y的信息增益定义为:<br>
$$IG(Y|X)=H(Y)-H(Y|X)$$
<br>假设存在特征子集A和特征子集B，分类变量为C，若IG( C|A ) &gt; IG( C|B ) ，则认为选用特征子集A的分类结果比B好，因此倾向于选用特征子集A。</p>
<p>(3)最大信息系数(Mutual information and maximal information coefficient,MIC)</p>
<p>Pearson相关系数不能反映变量之间的非线性关系；信息增益直接用于特征选择时通常变量需要先离散化，而且信息增益的结果对离散化的方式很敏感。但是最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 </p>
<p>MIC的性质：</p>
<ul>
<li>对于所有无噪声并且非常数的函数关系，MIC的值都为1</li>
<li>对于两个有确定的函数关系的随机变量，不论这个函数关系是什么样的，MIC的值都是1</li>
<li>对于两个统计独立的随机变量MIC的值接近于0</li>
</ul>
<p>Python中，minepy 提供了MIC功能。</p>
<p>反过头来看$y=x^2$这个例子，MIC算出来的互信息值为1(最大的取值)。</p>
<figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from minepy import MINE</span><br><span class="line">m = MINE()</span><br><span class="line"><span class="keyword">x</span> = np.random.normal(<span class="number">0</span>,<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line">m.compute_score(<span class="keyword">x</span>, <span class="keyword">x</span>**<span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> m.mic()</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>R中minerva包的mine函数可直计算出mic的值。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">library</span><span class="params">(minerva)</span></span></span><br><span class="line">x &lt;- rnorm(<span class="number">100</span>)</span><br><span class="line"><span class="function"><span class="title">mine</span><span class="params">(x,y=x^<span class="number">2</span>)</span></span><span class="variable">$MIC</span></span><br><span class="line">[<span class="number">1</span>] <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>可以看出，虽然x与$y=x^2$是函数关系，Pearson相关系数却很相小，但是通过MIC为1可以发现二者之间强烈的非线性关系。</p>
<p>(4)距离相关系数 (Distance correlation)</p>
<p>距离相关系数是针对 Pearson 相关系数只能表征线性关系的缺点而提出的。其思想是分别构建特征变量和响应变量的欧氏距离矩阵，并由此计算特征变量和响应变量的距离相关系数。详细的定义和计算过程可参考维基百科。</p>
<p>距离相关系数能够同时捕捉变量之间的线性和非线性相关。当距离相关系数为 0，则可断言两个变量相互独立（Pearson 相关系数为 0 不代表变量相互独立）。其缺点是与 Pearson 相关系数相比，其所需的运算量较大，而且取值范围为 [0, 1]，无法表征变量之间关联是正相关还是负相关。</p>
<p>R的 energy 包里提供了距离相关系数的实现</p>
<figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">x</span> &lt;- rnorm(<span class="number">100</span>)</span><br><span class="line">dcor(<span class="keyword">x</span>, <span class="keyword">x</span>**<span class="number">2</span>)</span><br><span class="line"><span class="number">0.5880842</span></span><br></pre></td></tr></table></figure>
<p>(5)WOE和IV</p>
<p>WOE是一种基于目标变量的自变量编码方式，信用评分卡模型中最常用这种编码方式。例如将模型目标标量为1记为违约用户，对于目标变量为0记为正常用户；则WOE(weight of Evidence)其实就是自变量取某个值的时候对违约比例的一种影响。</p>
<p>WOE的公式：$$woe_{i}=ln(\frac{p_{y=1}}{p_{y=0}})$$</p>
<p>IV的公式：$$IV_{i}=(p_{y=1}-p_{y=0})\times woe_{i}$$</p>
<p>那么， $$IV=\sum_{i=1}^{k}IV_{i}$$</p>
<p>例如下表：</p>
<table>
<thead>
<tr>
<th>变量(age)</th>
<th>bad(y=1人数)</th>
<th>good(y=0的人数)</th>
<th>woe</th>
<th>IV</th>
</tr>
</thead>
<tbody>
<tr>
<td>0-18</td>
<td>10</td>
<td>50</td>
<td>log((10/40)/(50/100))</td>
<td>{(10/40)-(50/100)}log((10/40)/(50/100))</td>
</tr>
<tr>
<td>18-30</td>
<td>10</td>
<td>20</td>
<td>log((10/40)/(20/100))</td>
<td>{(10/40)-(20/100)}log((10/40)/(20/100))</td>
</tr>
<tr>
<td>30-50</td>
<td>20</td>
<td>30</td>
<td>log((20/40)/(30/100))</td>
<td>{(20/40)-(30/100)}log((20/40)/(30/100))</td>
</tr>
<tr>
<td>sum</td>
<td>40</td>
<td>100</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>表中以age年龄为某个自变量，由于年龄是连续型自变量，需要对其进行离散化处理，假设离散化分为3组，bad和good表示在这三组中违约用户和正常用户的数量分布，可以计算出每一组的WOE值和IV值，通过后面的公式可以看出，woe反映的是在自变量每个分组下违约用户对正常用户占比和总体中违约用户对正常用户占比之间的差异；从而可以直观的认为woe蕴含了自变量取值对于目标变量（违约概率）的影响。</p>
<p>该变量age的IV值为：<br>
$$IV=\sum_{i=1}^{3}IV_{i}$$
</p>
<ul>
<li>从公式来看的话，其实IV衡量的是某一个变量的信息量，相当于是自变量woe值的一个加权求和，其值的大小决定了自变量对于目标变量的影响程度</li>
</ul>
<ul>
<li>从另一个角度来看的话，IV公式与信息熵的公式极其相似。</li>
</ul>
<ul>
<li>变量的IV值表示着该变量对因变量的区分度，IV值越大，该变量的越有价值，从而可以用IV值来进行特征选择。</li>
</ul>
<h4 id="2-2-2-包裹式-Wrapper"><a href="#2-2-2-包裹式-Wrapper" class="headerlink" title="2.2.2 包裹式(Wrapper)"></a>2.2.2 包裹式(Wrapper)</h4><p>而把特征选择和模型优化的标准统一起来的方法则有包裹式（wrapper）和嵌入式（embedding）。包裹式的方法以模型的优化标准作为特征选择的标准，但仍然把特征选择和模型训练分为两个步骤。与过滤式特征选择不考虑后续学习器不同，包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价标准，比如准确率、召回率、AUC值、AIC 准则和 BIC 准则等评价标准。</p>
<ul>
<li>赤池信息准则（Akaike information criterion, AIC）<br>$$AIC=2k-2ln(L)$$</li>
</ul>
<ul>
<li>贝叶斯信息准则（Bayesian Information Criterions, BIC）<br>$$BIC=ln(n)*k-2ln(L)$$</li>
</ul>
<p>其中k为参数数目，$L$是似然函数（likelihood function）,n是数据中观测值的数量。</p>
<p>AIC 和 BIC 的表达式中均包含了模型复杂度惩罚项和最大似然函数项。不同的地方在于，在 BIC 的表达式中，惩罚项的权重随观测值的增加而增加。因此当观测值数量较大时，只有显著关联的特征变量才会被保留，从而降低模型的复杂性。</p>
<p>在建模时，我们可以通过最小化 AIC 或 BIC 来选择模型的最优参数。由表达式可以看出，AIC 和 BIC 倾向于复杂度低（越小越好）和符合先验假设（越大越好）的模型。在简单线性回归中，似然函数是依据残差服从正态分布的先验假设构建的，即如果特征变量的加入能够使残差更接近正态分布，则认为这个特征能够显著改善线性回归模型。</p>
<h4 id="2-2-3-嵌入式-Embedded"><a href="#2-2-3-嵌入式-Embedded" class="headerlink" title="2.2.3 嵌入式(Embedded)"></a>2.2.3 嵌入式(Embedded)</h4><p>嵌入式则是把特征选择和模型训练融为一体，两者在同一个优化的过程中完成，不再分为两个步骤，即在学习器训练过程中自动的进行了特征选择。最典型的有L1正则化以及决策树算法，如ID3、C4.5和CART算法等，决策树算法在树增长过程的每个递归步都必须选择一个特征，将样本集划分成较小的子集，选择特征的依据通常是划分后子节点的纯度，划分后子节点越纯，则说明划分效果越好，可见决策树生成的过程也就是特征选择的过程。</p>
<p>(1) L1正则化 </p>
<p>在优化理论中，正则化（regularization）是一类通过对解施加先验约束把不适定问题（ill-posed problem）转化为适定问题的常用技巧。例如，在线性回归模型中，当用最小二乘法估计线性回归的系数时，如果自变量存在共线性，系数的估计值将具有较大的方差，因而会影响后续参数的统计检验。如果在最小二乘法的参数估计表达式中添加L1正则项，则称为Lasso线性回归模型：</p>
<p>$$\widehat{\beta }=arg min_{\beta}[(Y-\beta X)^{T}(Y-\beta X)+\lambda|\beta|]$$</p>
<p>L1正则化将系数w的L1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。线性回归有Lasso回归，分类模型有L1逻辑回归。</p>
<p>(2) 基于树的模型</p>
<p>基于树的模型，如随机森林、GBDT、XGBOOST等都可以输出变量的重要度，从而达到特征选择的作用。</p>
<ul>
<li>平均不纯度减少 (mean decrease impurity)</li>
</ul>
<p>决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度或者信息增益 ，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。</p>
<ul>
<li>平均精确率减少 (Mean decrease accuracy)</li>
</ul>
<p>另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。</p>
<p>(3)递归特征消除 (Recursive feature elimination ,RFE)</p>
<p>递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一边，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。</p>
<p>RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。</p>
<p>Python 的 scikit-learn 模块中提供了一种循环特征剔除 (recursive feature elimination, RFE) 的实现，遵循的也是后向搜索的思路。</p>
<blockquote>
<h2 id="3-代码演练"><a href="#3-代码演练" class="headerlink" title="3  代码演练"></a>3  代码演练</h2></blockquote>
<h3 id="1-基于前向搜索、后向搜索和双向搜索的回归模型在R中的实现："><a href="#1-基于前向搜索、后向搜索和双向搜索的回归模型在R中的实现：" class="headerlink" title="(1) 基于前向搜索、后向搜索和双向搜索的回归模型在R中的实现："></a>(1) 基于前向搜索、后向搜索和双向搜索的回归模型在R中的实现：</h3><p> 这三类算法在R语言的stats包和MASS包中的线性回归建模中都实现了。</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">library(MASS)</span><br><span class="line">initial &lt;- glm(Class ~ tau + VEGF + E4 + IL_3, data = training, family = binomial)</span><br><span class="line">stepAIC(initial, direction = "both")</span><br><span class="line">Start: AIC=189.46</span><br><span class="line">Class ~ tau + VEGF + E4 + IL_3</span><br><span class="line"></span><br><span class="line">Df Deviance AIC</span><br><span class="line">-<span class="ruby"> IL_3 <span class="number">1</span> <span class="number">179.69</span> <span class="number">187.69</span></span><br><span class="line"></span>-<span class="ruby"> E4 <span class="number">1</span> <span class="number">179.72</span> <span class="number">187.72</span></span><br><span class="line"></span>&lt;none&gt; 179.46 189.46</span><br><span class="line">-<span class="ruby"> VEGF <span class="number">1</span> <span class="number">242.77</span> <span class="number">250.77</span></span><br><span class="line"></span>-<span class="ruby"> tau <span class="number">1</span> <span class="number">288.61</span> <span class="number">296.61</span></span><br><span class="line"></span></span><br><span class="line">Step: AIC=187.69</span><br><span class="line">Class ~ tau + VEGF + E4</span><br><span class="line"></span><br><span class="line">Df Deviance AIC</span><br><span class="line">-<span class="ruby"> E4 <span class="number">1</span> <span class="number">179.84</span> <span class="number">185.84</span></span><br><span class="line"></span>&lt;none&gt; 179.69 187.69</span><br><span class="line">+ IL_3 1 179.46 189.46</span><br><span class="line">-<span class="ruby"> VEGF <span class="number">1</span> <span class="number">248.30</span> <span class="number">254.30</span></span><br><span class="line"></span>-<span class="ruby"> tau <span class="number">1</span> <span class="number">290.05</span> <span class="number">296.05</span></span><br><span class="line"></span></span><br><span class="line">Step: AIC=185.84</span><br><span class="line">Class ~ tau + VEGF</span><br><span class="line"></span><br><span class="line">Df Deviance AIC</span><br><span class="line">&lt;none&gt; 179.84 185.84</span><br><span class="line">+ E4 1 179.69 187.69</span><br><span class="line">+ IL_3 1 179.72 187.72</span><br><span class="line">-<span class="ruby"> VEGF <span class="number">1</span> <span class="number">255.07</span> <span class="number">259.07</span></span><br><span class="line"></span>-<span class="ruby"> tau <span class="number">1</span> <span class="number">300.69</span> <span class="number">304.69</span></span><br><span class="line"></span></span><br><span class="line">Call: glm(formula = Class ~ tau + VEGF, family = binomial, data = training)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept) tau VEGF</span><br><span class="line">9.8075 -4.2779 0.9761</span><br><span class="line"></span><br><span class="line">Degrees of Freedom: 266 Total (i.e. Null); 264 Residual</span><br><span class="line">Null Deviance: 313.3</span><br><span class="line">Residual Deviance: 179.8 AIC: 185.8</span><br></pre></td></tr></table></figure>
<p>基于AIC准则，通过双向搜索我们得到了最优的分类模型。</p>
<h3 id="2-基于IV的特征变量选择在R中的实现："><a href="#2-基于IV的特征变量选择在R中的实现：" class="headerlink" title="(2) 基于IV的特征变量选择在R中的实现："></a>(2) 基于IV的特征变量选择在R中的实现：</h3><p>在R中，smbinning包可以做变量的分箱、计算woe以及IV。用其自带数据集举例如下：</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"># Package loading <span class="keyword">and</span> data exploration</span><br><span class="line">library(smbinning) # Load package <span class="keyword">and</span> its data</span><br><span class="line">data(chileancredit) # Load smbinning sample dataset (Chilean Credit)</span><br><span class="line">str(chileancredit) # Quick description of the data</span><br><span class="line"></span><br><span class="line"><span class="string">'data.frame'</span>:	<span class="number">7702</span> obs. of  <span class="number">19</span> <span class="keyword">variables</span>:</span><br><span class="line"> $ CustomerId       <span class="comment">: chr</span>  <span class="comment">"0000000185"</span> <span class="comment">"0000000238"</span> <span class="comment">"0000000346"</span> <span class="comment">"0000000460"</span><span class="comment"> ...</span></span><br><span class="line"> $ TOB              <span class="comment">: int  44 79 102 NA 109 183 172 76 136 171 ...</span></span><br><span class="line"> $ IncomeLevel      <span class="comment">: Factor w</span>/ <span class="number">6</span> levels <span class="comment">"0"</span>,<span class="comment">"1"</span>,<span class="comment">"2"</span>,<span class="comment">"3"</span>,..: <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="literal">NA</span> <span class="literal">NA</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> ...</span><br><span class="line"> $ Bal01            : num  <span class="number">605</span> <span class="number">1006</span> <span class="number">299</span> <span class="number">645</span> <span class="number">218</span> ...</span><br><span class="line"> $ MaxDqBin01       : Factor w/<span class="comment"> 7 levels</span> <span class="comment">"0"</span><span class="comment">,</span><span class="comment">"1"</span><span class="comment">,</span><span class="comment">"2"</span><span class="comment">,</span><span class="comment">"3"</span><span class="comment">,..: 1 1 1 1 1 1 1 1 1 1 ...</span></span><br><span class="line"> $ MaxDqBin02       <span class="comment">: Factor w</span>/ <span class="number">8</span> levels <span class="comment">"0"</span>,<span class="comment">"1"</span>,<span class="comment">"2"</span>,<span class="comment">"3"</span>,..: <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> ...</span><br><span class="line"> $ MaxDqBin03       : Factor w/<span class="comment"> 8 levels</span> <span class="comment">"0"</span><span class="comment">,</span><span class="comment">"1"</span><span class="comment">,</span><span class="comment">"2"</span><span class="comment">,</span><span class="comment">"3"</span><span class="comment">,..: 1 1 1 1 1 1 1 2 1 1 ...</span></span><br><span class="line"> $ MaxDqBin04       <span class="comment">: Factor w</span>/ <span class="number">8</span> levels <span class="comment">"0"</span>,<span class="comment">"1"</span>,<span class="comment">"2"</span>,<span class="comment">"3"</span>,..: <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> ...</span><br><span class="line"> $ MaxDqBin05       : Factor w/<span class="comment"> 8 levels</span> <span class="comment">"0"</span><span class="comment">,</span><span class="comment">"1"</span><span class="comment">,</span><span class="comment">"2"</span><span class="comment">,</span><span class="comment">"3"</span><span class="comment">,..: 1 1 1 1 1 1 1 1 1 1 ...</span></span><br><span class="line"> $ MaxDqBin06       <span class="comment">: Factor w</span>/ <span class="number">8</span> levels <span class="comment">"0"</span>,<span class="comment">"1"</span>,<span class="comment">"2"</span>,<span class="comment">"3"</span>,..: <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> ...</span><br><span class="line"> $ MtgBal01         : num  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> ...</span><br><span class="line"> $ NonBankTradesDq01: int  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> ...</span><br><span class="line"> $ NonBankTradesDq02: int  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> ...</span><br><span class="line"> $ NonBankTradesDq03: int  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> ...</span><br><span class="line"> $ NonBankTradesDq04: int  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> ...</span><br><span class="line"> $ NonBankTradesDq05: int  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> ...</span><br><span class="line"> $ NonBankTradesDq06: int  <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> ...</span><br><span class="line"> $ FlagGB           : int  <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> ...</span><br><span class="line"> $ FlagSample       : int  <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> ...</span><br><span class="line"> </span><br><span class="line"><span class="keyword">table</span>(chileancredit$FlagGB) # Tabulate target variable</span><br><span class="line"></span><br><span class="line"> <span class="number">0</span>    <span class="number">1</span> </span><br><span class="line"> <span class="number">636</span> <span class="number">5654</span></span><br><span class="line"> </span><br><span class="line"># Training <span class="keyword">and</span> testing samples (Just some basic formality <span class="keyword">for</span> Modeling)</span><br><span class="line">chileancredit.train=subset(chileancredit,FlagSample==<span class="number">1</span>)</span><br><span class="line">chileancredit.test=subset(chileancredit,FlagSample==<span class="number">0</span>)</span><br><span class="line"># Package application</span><br><span class="line">result=smbinning(df=chileancredit.train,y=<span class="comment">"FlagGB"</span>,x=<span class="comment">"TOB"</span>,p=<span class="number">0.05</span>) # Run <span class="keyword">and</span> save result</span><br><span class="line">result$ivtable # Tabulation <span class="keyword">and</span> Information Value</span><br><span class="line"></span><br><span class="line">Cutpoint CntRec CntGood CntBad CntCumRec CntCumGood CntCumBad PctRec GoodRate</span><br><span class="line"><span class="number">1</span>    &lt;= <span class="number">17</span>   <span class="number">1138</span>     <span class="number">909</span>    <span class="number">229</span>      <span class="number">1138</span>        <span class="number">909</span>       <span class="number">229</span> <span class="number">0.2405</span>   <span class="number">0.7988</span></span><br><span class="line"><span class="number">2</span>    &lt;= <span class="number">30</span>    <span class="number">621</span>     <span class="number">530</span>     <span class="number">91</span>      <span class="number">1759</span>       <span class="number">1439</span>       <span class="number">320</span> <span class="number">0.1312</span>   <span class="number">0.8535</span></span><br><span class="line"><span class="number">3</span>    &lt;= <span class="number">63</span>   <span class="number">1064</span>     <span class="number">985</span>     <span class="number">79</span>      <span class="number">2823</span>       <span class="number">2424</span>       <span class="number">399</span> <span class="number">0.2249</span>   <span class="number">0.9258</span></span><br><span class="line"><span class="number">4</span>     &gt; <span class="number">63</span>   <span class="number">1427</span>    <span class="number">1377</span>     <span class="number">50</span>      <span class="number">4250</span>       <span class="number">3801</span>       <span class="number">449</span> <span class="number">0.3016</span>   <span class="number">0.9650</span></span><br><span class="line"><span class="number">5</span>  Missing    <span class="number">482</span>     <span class="number">435</span>     <span class="number">47</span>      <span class="number">4732</span>       <span class="number">4236</span>       <span class="number">496</span> <span class="number">0.1019</span>   <span class="number">0.9025</span></span><br><span class="line"><span class="number">6</span>    Total   <span class="number">4732</span>    <span class="number">4236</span>    <span class="number">496</span>        <span class="literal">NA</span>         <span class="literal">NA</span>        <span class="literal">NA</span> <span class="number">1.0000</span>   <span class="number">0.8952</span></span><br><span class="line">  BadRate    Odds LnOdds     WoE     IV</span><br><span class="line"><span class="number">1</span>  <span class="number">0.2012</span>  <span class="number">3.9694</span> <span class="number">1.3786</span> <span class="number">-0.7662</span> <span class="number">0.1893</span></span><br><span class="line"><span class="number">2</span>  <span class="number">0.1465</span>  <span class="number">5.8242</span> <span class="number">1.7620</span> <span class="number">-0.3828</span> <span class="number">0.0223</span></span><br><span class="line"><span class="number">3</span>  <span class="number">0.0742</span> <span class="number">12.4684</span> <span class="number">2.5232</span>  <span class="number">0.3784</span> <span class="number">0.0277</span></span><br><span class="line"><span class="number">4</span>  <span class="number">0.0350</span> <span class="number">27.5400</span> <span class="number">3.3156</span>  <span class="number">1.1708</span> <span class="number">0.2626</span></span><br><span class="line"><span class="number">5</span>  <span class="number">0.0975</span>  <span class="number">9.2553</span> <span class="number">2.2252</span>  <span class="number">0.0804</span> <span class="number">0.0006</span></span><br><span class="line"><span class="number">6</span>  <span class="number">0.1048</span>  <span class="number">8.5403</span> <span class="number">2.1448</span>  <span class="number">0.0000</span> <span class="number">0.5025</span></span><br><span class="line"></span><br><span class="line"># Summary IV application</span><br><span class="line">sumivt=smbinning.sumiv(chileancredit.train,y=<span class="comment">"FlagGB"</span>)</span><br><span class="line">sumivt # <span class="keyword">Display</span> <span class="keyword">table</span> with IV by characteristic</span><br><span class="line"></span><br><span class="line">  Char     IV                  Process</span><br><span class="line"><span class="number">5</span>         MaxDqBin01 <span class="number">2.3771</span>        Factor binning OK</span><br><span class="line"><span class="number">6</span>         MaxDqBin02 <span class="number">1.8599</span>        Factor binning OK</span><br><span class="line"><span class="number">12</span> NonBankTradesDq01 <span class="number">1.8129</span>       Numeric binning OK</span><br><span class="line"><span class="number">13</span> NonBankTradesDq02 <span class="number">1.4417</span>       Numeric binning OK</span><br><span class="line"><span class="number">7</span>         MaxDqBin03 <span class="number">1.3856</span>        Factor binning OK</span><br><span class="line"><span class="number">14</span> NonBankTradesDq03 <span class="number">1.1819</span>       Numeric binning OK</span><br><span class="line"><span class="number">8</span>         MaxDqBin04 <span class="number">1.0729</span>        Factor binning OK</span><br><span class="line"><span class="number">15</span> NonBankTradesDq04 <span class="number">0.8948</span>       Numeric binning OK</span><br><span class="line"><span class="number">9</span>         MaxDqBin05 <span class="number">0.8844</span>        Factor binning OK</span><br><span class="line"><span class="number">16</span> NonBankTradesDq05 <span class="number">0.7511</span>       Numeric binning OK</span><br><span class="line"><span class="number">10</span>        MaxDqBin06 <span class="number">0.6302</span>        Factor binning OK</span><br><span class="line"><span class="number">17</span> NonBankTradesDq06 <span class="number">0.5501</span>       Numeric binning OK</span><br><span class="line"><span class="number">2</span>                TOB <span class="number">0.5025</span>       Numeric binning OK</span><br><span class="line"><span class="number">3</span>        IncomeLevel <span class="number">0.3380</span>        Factor binning OK</span><br><span class="line"><span class="number">11</span>          MtgBal01 <span class="number">0.1452</span>       Numeric binning OK</span><br><span class="line"><span class="number">4</span>              Bal01 <span class="number">0.1379</span>       Numeric binning OK</span><br><span class="line"><span class="number">1</span>         CustomerId     <span class="literal">NA</span>   <span class="keyword">Not</span> numeric nor factor</span><br><span class="line"><span class="number">18</span>        FlagSample     <span class="literal">NA</span> Uniques values of x &lt; <span class="number">10</span></span><br><span class="line"></span><br><span class="line"># Plotting smbinning.sumiv</span><br><span class="line">smbinning.sumiv.plot(sumivt,cex=<span class="number">0.8</span>) # Plot IV summary <span class="keyword">table</span></span><br></pre></td></tr></table></figure>
<p><img src="/img/iv.png" alt="IV"></p>
<p>通过IV的大小进行特征排序，从而达到特征选择的目的。</p>
<h3 id="3-基于L1正则化的特征变选择在R中的实现："><a href="#3-基于L1正则化的特征变选择在R中的实现：" class="headerlink" title="(3) 基于L1正则化的特征变选择在R中的实现："></a>(3) 基于L1正则化的特征变选择在R中的实现：</h3><p>R语言中glmnet包可以实现L1正则化、L2正则化的分类与回归模型。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">library(glmnet)</span><br><span class="line">library(ISLR)</span><br><span class="line">Hitters=na.omit(Hitters)</span><br><span class="line">x=model.<span class="built_in">matrix</span>(Salary~.,data=Hitters)[,-<span class="number">1</span>]</span><br><span class="line">y=Hitters$Salary</span><br><span class="line"><span class="built_in">grid</span>=<span class="number">10</span>^seq(<span class="number">10</span>,-<span class="number">2</span>,<span class="built_in">length</span>=<span class="number">100</span>)</span><br><span class="line">lasso.model=glmnet(x,y,alpha=<span class="number">1</span>,<span class="built_in">lambda</span>=<span class="built_in">grid</span>)</span><br><span class="line">#交叉验证，选择最优的<span class="built_in">lambda</span></span><br><span class="line"><span class="built_in">cv</span>.out=<span class="built_in">cv</span>.glmnet(x,y,alpha=<span class="number">1</span>,<span class="built_in">lambda</span>=<span class="built_in">grid</span>)</span><br><span class="line">bestlam=<span class="built_in">cv</span>.out$<span class="built_in">lambda</span>.<span class="built_in">min</span></span><br><span class="line">lasso.model2=glmnet(x,y,alpha=<span class="number">1</span>,<span class="built_in">lambda</span>=bestlam)</span><br><span class="line">coef(lasso.model2)</span><br><span class="line"></span><br><span class="line"><span class="number">20</span> x <span class="number">1</span> <span class="built_in">sparse</span> Matrix of class <span class="string">"dgCMatrix"</span></span><br><span class="line">                     s0</span><br><span class="line">(Intercept)  <span class="number">24.6396990</span></span><br><span class="line">AtBat         .        </span><br><span class="line">Hits          <span class="number">1.8499834</span></span><br><span class="line">HmRun         .        </span><br><span class="line">Runs          .        </span><br><span class="line">RBI           .        </span><br><span class="line">Walks         <span class="number">2.1959762</span></span><br><span class="line">Years         .        </span><br><span class="line">CAtBat        .        </span><br><span class="line">CHits         .        </span><br><span class="line">CHmRun        .        </span><br><span class="line">CRuns         <span class="number">0.2058514</span></span><br><span class="line">CRBI          <span class="number">0.4095910</span></span><br><span class="line">CWalks        .        </span><br><span class="line">LeagueN       .        </span><br><span class="line">DivisionW   -<span class="number">99.9733911</span></span><br><span class="line">PutOuts       <span class="number">0.2158910</span></span><br><span class="line">Assists       .        </span><br><span class="line">Errors        .        </span><br><span class="line">NewLeagueN     .</span><br></pre></td></tr></table></figure>
<p>通过这个实验可以看出，lasso的系数结果是稀疏的，19个预测变量中13个的系数为0。即使用交叉验证选择$\lambda$值建立的lasso模型仅包含6个预测变量。</p>
<h3 id="4-基于树模型的特征选择在R中的实现："><a href="#4-基于树模型的特征选择在R中的实现：" class="headerlink" title="(4) 基于树模型的特征选择在R中的实现："></a>(4) 基于树模型的特征选择在R中的实现：</h3><p>在R语言中，基于树的模型，如随机森林、GBDT、XGBOOST等都可以输出变量的重要度，以随机森林为例。</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(<span class="name">randomForest</span>)</span><br><span class="line">library(<span class="name">mlbench</span>)  </span><br><span class="line"># load the data  </span><br><span class="line">data(<span class="name">PimaIndiansDiabetes</span>) </span><br><span class="line">rf &lt;- randomForest(<span class="name">x=PimaIndiansDiabetes</span>[,<span class="number">1</span>:<span class="number">8</span>], </span><br><span class="line">          y=PimaIndiansDiabetes[,<span class="number">9</span>],importance = T,ntree=500) </span><br><span class="line"></span><br><span class="line">barplot(<span class="name">rf</span>$importance[,<span class="number">3</span>],main=<span class="string">"输入重要性指标的柱形图"</span>)</span><br><span class="line">varImpPlot(<span class="name">rf</span>,sort=TRUE,n.var=nrow(<span class="name">rf</span>$importance))</span><br></pre></td></tr></table></figure>
<p><img src="/img/bar.png" alt="输入重要性指标"></p>
<p><img src="/img/rf.png" alt="平均精确率减少与平均不纯度减少"></p>
<p>随机森林是基于决策树的集成模型，通过上图，我们可以很清楚地发现重要的特征。</p>
<p>###　(5)基于递归特征消除法的特征选择在R中的实现：</p>
<p>在R中，caret包可以使用递归特征消除法，主要使用rfe函数。</p>
<p>rfe参数说明：</p>
<p>x，预测变量的矩阵或数据框</p>
<p>y，输出结果向量（数值型或因子型）</p>
<p>sizes，用于测试的特定子集大小的整型向量</p>
<p>rfeControl，用于指定预测模型和方法的一系列选项</p>
<p>一些列函数可以用于rfeControl$functions，包括：线性回归（lmFuncs），随机森林（rfFuncs），朴素贝叶斯(nbFuncs)，bagged trees（treebagFuncs)和可以用于caret的train函数的函数（caretFuncs）。</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#ensure the results are repeatable  </span><br><span class="line"><span class="keyword">set</span>.seed(7)  </span><br><span class="line">#load <span class="comment">the library</span>  </span><br><span class="line">library(mlbench)  </span><br><span class="line">library(caret)  </span><br><span class="line"># load <span class="comment">the data</span>  </span><br><span class="line">data(PimaIndiansDiabetes)  </span><br><span class="line">#define <span class="comment">the control using a random forest selection function</span>  </span><br><span class="line">control <span class="comment">&lt;- rfeControl(functions=rfFuncs, method=</span><span class="comment">"cv"</span><span class="comment">, number=10)</span>  </span><br><span class="line"># run <span class="comment">the RFE algorithm</span>  </span><br><span class="line">results <span class="comment">&lt;- rfe(PimaIndiansDiabetes[,1:8],</span> </span><br><span class="line">               PimaIndiansDiabetes[,9], sizes=c(1:8), </span><br><span class="line">               rfeControl=control,metric <span class="comment">=</span> <span class="comment">"Accuracy"</span><span class="comment">,ntree=100)</span>  </span><br><span class="line">#summarize <span class="comment">the results</span>  </span><br><span class="line">print(results)  </span><br><span class="line"></span><br><span class="line">Recursive <span class="comment">feature selection</span></span><br><span class="line">Outer <span class="comment">resampling method: Cross-Validated (10 fold)</span> </span><br><span class="line">Resampling <span class="comment">performance over subset size:</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">Variables</span> <span class="comment">Accuracy  Kappa AccuracySD KappaSD Selected</span></span><br><span class="line">         1   0.7082 0.3030    0.04902  0.1181         </span><br><span class="line">         2   0.7330 0.3920    0.06703  0.1564         </span><br><span class="line">         3   0.7409 0.4115    0.06525  0.1582         </span><br><span class="line">         4   0.7512 0.4409    0.06988  0.1691         </span><br><span class="line">         5   0.7553 0.4452    0.07909  0.1911         </span><br><span class="line">         6   0.7618 0.4583    0.08094  0.1897         </span><br><span class="line">         7   0.7631 0.4620    0.08448  0.1927        *</span><br><span class="line">         8   0.7617 0.4615    0.07810  0.1789         </span><br><span class="line"></span><br><span class="line">The <span class="comment">top 5 variables (out of 7):</span></span><br><span class="line">   glucose, mass, age, pregnant, pedigree</span><br><span class="line">#list <span class="comment">the chosen features</span>  </span><br><span class="line">predictors(results)  </span><br><span class="line"></span><br><span class="line">[1] <span class="string">"glucose"</span>  <span class="string">"mass"</span>     <span class="string">"age"</span>      <span class="string">"pregnant"</span> <span class="string">"pedigree"</span> <span class="string">"insulin"</span>  <span class="string">"triceps"</span></span><br><span class="line"></span><br><span class="line">#plot <span class="comment">the results</span>  </span><br><span class="line">plot(results, type=c(<span class="string">"g"</span>, <span class="string">"o"</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/img/rfe.png" alt="image"></p>
<p>从案例中可以看出，通过递归特征消除，发现用所有变量中的某7个自变量的模型是最好的，并且找出了最重要的5个特征。</p>
<blockquote>
<h2 id="4-结束语"><a href="#4-结束语" class="headerlink" title="4  结束语"></a>4  结束语</h2></blockquote>
<p>特征工程是门艺术，也是每一个数据科学家的必修课，因此每个数据科学家既应该像一个统计学家和工程师，又应该像一个艺术家。</p>
<blockquote>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3></blockquote>
<p>[1] Dash M, Liu H. Feature selection for classification[J]. Intelligent data analysis, 1997, 1(3): 131-156.</p>
<p>[2]<a href="http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/" target="_blank" rel="external">http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/</a></p>
<p>[3]<a href="http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/" target="_blank" rel="external">http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/</a></p>
<p>[4]<a href="http://blog.datadive.net/selecting-good-features-part-iii-random-forests/" target="_blank" rel="external">http://blog.datadive.net/selecting-good-features-part-iii-random-forests/</a></p>
<p>[5]<a href="http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/" target="_blank" rel="external">http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/</a></p>
<p>[6]<a href="http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection" target="_blank" rel="external">http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection</a></p>
<p>[7]<a href="http://blog.csdn.net/qtlyx/article/details/50780400" target="_blank" rel="external">http://blog.csdn.net/qtlyx/article/details/50780400</a></p>
<p>[8]<a href="http://minepy.readthedocs.io/en/latest/" target="_blank" rel="external">http://minepy.readthedocs.io/en/latest/</a></p>
<p>[9]<a href="https://cran.r-project.org/web/packages/minerva/minerva.pdf" target="_blank" rel="external">https://cran.r-project.org/web/packages/minerva/minerva.pdf</a></p>
<p>[10]<a href="http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/" target="_blank" rel="external">http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/</a></p>
<p>[11]<a href="http://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/" target="_blank" rel="external">http://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/</a></p>
<p>[12]<a href="https://www.zhihu.com/question/28641663" target="_blank" rel="external">https://www.zhihu.com/question/28641663</a></p>
<p>[13]<a href="https://cran.r-project.org/web/packages/smbinning/smbinning.pdf" target="_blank" rel="external">https://cran.r-project.org/web/packages/smbinning/smbinning.pdf</a></p>
<p>[14]<a href="http://www.scoringmodeling.com/rpackage/smbinning/" target="_blank" rel="external">http://www.scoringmodeling.com/rpackage/smbinning/</a></p>
<p>[15]蒋杭进. 最大信息系数及其在脑网络分析中的应用[D]. 中国科学院研究生院 (武汉物理与数学研究所), 2013.</p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2016/09/27/特征选择/">Feature Selection</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">yunxileo</a></p>
        <p><span>发布时间:</span>2016-09-27, 15:35:15</p>
        <p><span>最后更新:</span>2017-02-06, 10:54:49</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2016/09/27/特征选择/" title="Feature Selection">http://yunxileo.github.io/2016/09/27/特征选择/</a>
            <span class="copy-path" data-clipboard-text="原文: http://yunxileo.github.io/2016/09/27/特征选择/　　作者: yunxileo" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2017/02/06/improve-R-code-efficiency/">
                    improve R code efficiency
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2016/05/19/缺失值处理/">
                    缺失值处理
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-特征选择的概述"><span class="toc-number">1.</span> <span class="toc-text">1.特征选择的概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-特征工程的简介"><span class="toc-number">1.1.</span> <span class="toc-text">1.1  特征工程的简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-特征选择的简介"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 特征选择的简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-特征选择的过程"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 特征选择的过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-特征选择常用的方法"><span class="toc-number">2.</span> <span class="toc-text">2.特征选择常用的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-基于特征子集的形成方式的特征选择方法"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 基于特征子集的形成方式的特征选择方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-基于评价准测的特征选择方法"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 基于评价准测的特征选择方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-过滤式-Filter"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.2.1 过滤式(Filter)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-包裹式-Wrapper"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2.2 包裹式(Wrapper)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-嵌入式-Embedded"><span class="toc-number">2.2.3.</span> <span class="toc-text">2.2.3 嵌入式(Embedded)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-代码演练"><span class="toc-number">3.</span> <span class="toc-text">3  代码演练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-基于前向搜索、后向搜索和双向搜索的回归模型在R中的实现："><span class="toc-number">3.1.</span> <span class="toc-text">(1) 基于前向搜索、后向搜索和双向搜索的回归模型在R中的实现：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-基于IV的特征变量选择在R中的实现："><span class="toc-number">3.2.</span> <span class="toc-text">(2) 基于IV的特征变量选择在R中的实现：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-基于L1正则化的特征变选择在R中的实现："><span class="toc-number">3.3.</span> <span class="toc-text">(3) 基于L1正则化的特征变选择在R中的实现：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-基于树模型的特征选择在R中的实现："><span class="toc-number">3.4.</span> <span class="toc-text">(4) 基于树模型的特征选择在R中的实现：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-结束语"><span class="toc-number">4.</span> <span class="toc-text">4  结束语</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#参考文献"><span class="toc-number">4.1.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol>
</div>
<style>
    .left-col .switch-btn {
        display: none;
    }
    .left-col .switch-area {
        display: none;
    }
</style>

<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">
<script>
    var valueHide = "隐藏目录";
    var valueShow = "显示目录";

    if ($(".left-col").is(":hidden")) {
        $("#tocButton").attr("value", valueShow);
    }

    $("#tocButton").click(function() {
        if ($("#toc").is(":hidden")) {
            $("#tocButton").attr("value", valueHide);
            $("#toc").slideDown(320);
            $(".switch-btn, .switch-area").fadeOut(300);
        }
        else {
            $("#tocButton").attr("value", valueShow);
            $("#toc").slideUp(350);
            $(".switch-btn, .switch-area").fadeIn(500);
        }
    })

    if ($(".toc").length < 1) {
        $("#toc, #tocButton").hide();
        $(".switch-btn, .switch-area").show();
    }
</script>





    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"Feature Selection　| 云稀里　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    
      <div class="duoshuo" id="comments">
    <!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="2016/09/27/特征选择/" data-title="Feature Selection" data-url="http://yunxileo.github.io/2016/09/27/特征选择/"></div>
    <!-- 多说评论框 end -->
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:"yunxileo"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
    </script>
    <!-- 多说公共JS代码 end -->
</div>

    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2017/02/06/improve-R-code-efficiency/" title="上一篇: improve R code efficiency">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2016/05/19/缺失值处理/" title="下一篇: 缺失值处理">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/02/06/improve-R-code-efficiency/">improve R code efficiency</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/27/特征选择/">Feature Selection</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/19/缺失值处理/">缺失值处理</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/16/逻辑回归的前世今生/">逻辑回归的前世今生</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2017 yunxileo
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >本站到访数: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">本页阅读量: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.1.22/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>




<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>