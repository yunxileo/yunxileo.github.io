<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>云稀里</title>
  <subtitle>莫厌追欢笑语频，寻思离乱好伤神。闲来屈指从头数，得见清平有几人</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yunxileo.github.io/"/>
  <updated>2016-05-19T09:43:20.913Z</updated>
  <id>http://yunxileo.github.io/</id>
  
  <author>
    <name>yunxileo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>缺失值处理</title>
    <link href="http://yunxileo.github.io/2016/05/19/%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/"/>
    <id>http://yunxileo.github.io/2016/05/19/缺失值处理/</id>
    <published>2016-05-19T09:37:41.000Z</published>
    <updated>2016-05-19T09:43:20.913Z</updated>
    
    <content type="html">&lt;h2 id=&quot;数据缺失的概念&quot;&gt;&lt;a href=&quot;#数据缺失的概念&quot; class=&quot;headerlink&quot; title=&quot;数据缺失的概念&quot;&gt;&lt;/a&gt;数据缺失的概念&lt;/h2&gt;&lt;p&gt;数据缺失是指在数据采集时由于某种原因应该得到而没有得到的数据。它指的是现有数据集中某个或某些属性的值是不完全的 。&lt;/p&gt;
&lt;h2 id=&quot;数据缺失机制&quot;&gt;&lt;a href=&quot;#数据缺失机制&quot; class=&quot;headerlink&quot; title=&quot;数据缺失机制&quot;&gt;&lt;/a&gt;数据缺失机制&lt;/h2&gt;&lt;p&gt;在对缺失数据进行处理前，了解数据缺失的机制和形式是十分必要的。将数据集中不含缺失值的变量（属性）称为完全变量，数据集中含有缺失值的变量称为不完全变量。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;完全随机缺失（Missing Completely At Random,MCAR）：数据的缺失与不完全变量以及完全变量都是无关的。换句话说，某变量缺失值的出现完全是个随机事件。可以将存在MCAR变量的数据看做是假定完整数据的一个随机样本。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;随机缺失（Missing At Random,MAR）：数据的缺失不是完全随机的，数据的缺失只依赖于完全变量。例如，在一次测试中，如果IQ达不到最低要求的100分，那么将不能参加随后的人格测验。在人格测验上因为IQ低于100分而产生的缺失值为MAR。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;完全非随机缺失(Missing Not At Random,MNAR)：数据的缺失与完全变量自身有关，这种缺失是不可忽略的。例如，公司新录用了20名员工，由于6名员工表现较差在试用期内辞退，试用期结束后的表现评定中，辞退的6名员工的表现分即为非随机缺失。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从缺失值的所属属性上讲，如果所有的缺失值都是同一属性，那么这种缺失成为单值缺失，如果缺失值属于不同的属性，称为任意缺失。另外对于时间序列类的数据，可能存在随着时间的缺失，这种缺失称为单调缺失。&lt;/p&gt;
&lt;h2 id=&quot;数据缺失的处理方法&quot;&gt;&lt;a href=&quot;#数据缺失的处理方法&quot; class=&quot;headerlink&quot; title=&quot;数据缺失的处理方法&quot;&gt;&lt;/a&gt;数据缺失的处理方法&lt;/h2&gt;&lt;p&gt;缺失值处理的方法大致分为这几类：1、删除法；2、加权方法；3、基于插补的方法；4、基于模型的方法。&lt;/p&gt;
&lt;h3 id=&quot;删除法&quot;&gt;&lt;a href=&quot;#删除法&quot; class=&quot;headerlink&quot; title=&quot;删除法&quot;&gt;&lt;/a&gt;删除法&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;删除观察样本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将存在缺失数据的样本删除，从而得到一个完备的数据集。这种方法简单易行，在对象有多个属性缺失值、被删除的含缺失值的对象与信息表中的数据量相比非常小的情况下是非常有效的。但是，在数据集中本来包含的样本很少的情况下，删除少量对象就足以严重影响数据的客观性和结果的正确性，因此，当缺失数据所占比例较大，特别当缺失数据是非随机分布时，这种方法可能导致数据发生偏离，从而导致错误的结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;删除变量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当某个变量缺失值较多，且对研究目标影响不大，可以将变量整体删除。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;改变权重&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差。&lt;/p&gt;
&lt;p&gt;删除法具有很大的局限性，它是以减少历史数据来换取信息的完备，会造成资源的大量浪费，丢弃了大量隐藏在这些对象中的信息。&lt;/p&gt;
&lt;h3 id=&quot;加权方法&quot;&gt;&lt;a href=&quot;#加权方法&quot; class=&quot;headerlink&quot; title=&quot;加权方法&quot;&gt;&lt;/a&gt;加权方法&lt;/h3&gt;&lt;p&gt;在进行抽样调查时，被调查者拒绝配合，会造成不响应调查现象。无不响应的抽样调查数据的随机化的推断，通常用它们的设计权加权抽取的单元。设计权是反比于它们的选取概率。设$y_{i}$是变量Y在总体中单元i的值，总体均值常用Horvitz-Thompson估计量进行估计&lt;br&gt;$$(\sum_{i=1}^{n} \pi_{i}^{-1} y_{i})(\sum_{i=1}^{n} \pi_{i}^{-1})^{-1}$$&lt;br&gt;其中$\pi_i$是单元i进入样本的已知概率，对不响应的加权方法是调整权以适应抽样设计时预料到的不响应。用&lt;br&gt;
$$(\sum_{i=1}^{n}(\pi_{i} \hat{p}_{i})^{-1} y_{i} /\sum_{i=1}^{n}(\pi_{i} \hat{p}_i)^{-1}$$&lt;br&gt;替代Horvitz-Thompson估计量。${\hat{p}}_{i}$是单元i响应的概率的一个估计，通常它是样本的一个子类中的响应单元比例。&lt;/p&gt;
&lt;h3 id=&quot;基于插补的方法&quot;&gt;&lt;a href=&quot;#基于插补的方法&quot; class=&quot;headerlink&quot; title=&quot;基于插补的方法&quot;&gt;&lt;/a&gt;基于插补的方法&lt;/h3&gt;&lt;h4 id=&quot;单一插补&quot;&gt;&lt;a href=&quot;#单一插补&quot; class=&quot;headerlink&quot; title=&quot;单一插补&quot;&gt;&lt;/a&gt;单一插补&lt;/h4&gt;&lt;p&gt;单一插补指对每个缺失值，从其预测分布中取一个值填充缺失值后，使用标准的完全数据进行分析处理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;均值插补&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;均值插补是在处理数据时可以把变量分为数值型和非数值型，如果是非数值型的缺失数据，运用统计学中众数的原理，用此变量在其他对象中取值频数最多的值来填充缺失值；如果是数值型的缺失值，则取此变量在其他所有对象的取值均值来补齐缺失值，这种方法只能在缺失值是完全随机缺失时为总体均值或总量提供无偏估计。但此方法使得插补值集中在均值点上，在分布上容易形成尖峰，导致方差被低估。可根据一定的辅助变量，将样本分成多个部分，然后在每一部分上分别使用均值插补，称为局部均值插补。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;利用哑变量进行调整&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对缺失值创建一个指标，即设一个哑变量，1表示观测数据中存在缺失值，0表示观测数据中不存在缺失值，对缺失数据进行特定值的插补（如均值插补），这样做的好处是在缺失值处理时使用了全部变量的信息，但这样会导致估计有偏。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;热平台插补&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于一个包含空值的变量，本方法是在完整数据中找到一个与空值最相似的变量，然后用这个相似的值来进行填充。与均值插补法相比，热平台插补简单易懂还可以保持数据本身的类型，利用本方法填充数据后，其变量值与填充前很接近。它的主要缺点是不能覆盖已有数据没有反应的信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;冷平台插补&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;冷平台插补与热平台插补类似，但它是从过去的调查数据（如前期的调查数据或历史数据）中获得信息进行插补。冷平台插补同样不能消除估计的偏差。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;k-means插补&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;利用辅助变量（即无缺失值的变量），定义样本间的距离函数，寻找与缺失值样本距离最近的无缺失值的n个样本，利用这n个样本的加权平均值来估计缺失数据。即利用聚类模型预测缺失变量的类型，再以该类型的均值进行插补，但这种方法在模型中引入了自相关，容易给后面的分析工作造成障碍。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;期望最大化（EM算法）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该算法的特点是通过数据扩张，将不完全数据的处理问题转化为对完全数据的处理问题，且通过假设隐变量的存在，简化似然方程，将比较复杂的似然函数极大似然估计问题转化为比较简单的极大似然估计问题。通过以下步骤实现：1、用估计值替代缺失值；2、参数估计；3、假定2中的参数估计值是正确的，再对缺失值进行估计；4、再估计缺失值。&lt;/p&gt;
&lt;h4 id=&quot;随机插补&quot;&gt;&lt;a href=&quot;#随机插补&quot; class=&quot;headerlink&quot; title=&quot;随机插补&quot;&gt;&lt;/a&gt;随机插补&lt;/h4&gt;&lt;p&gt;随机插补是在均值插补的基础上加上随机项，通过增加缺失值的随机性来改善缺失值分布过于集中的缺陷。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;贝叶斯Boostap方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;贝叶斯Boostap方法主要包括两步：1、从（0,1）均匀分布中随机抽取k-1个随机数，并进行排序记为$a_{1},a_{2},\cdots,a_{k-1}$，并且使得$a_{0}=0$且$a_{k}=1$，其中k是观测值的数目；2、对n个缺失值，分别从观测数据$x_{1},\cdots,x_{k}$中以概率$(a_{1}-a_{0}),(a_{2}-a_{1}),\cdots,(1-a_{k-1})$抽取一个值进行插补。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;近似贝叶斯Boostap方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;近似贝叶斯Boostap方法首先从k个观测数据集$x_{1},\cdots,x_{k}$中有放回的抽取$k^{&lt;em&gt;}$个观测值建立一个新的数据集$x^{&lt;/em&gt;}$，对于n个缺失值，从数据集$x^{*}$中随机抽取n个值进行插补。&lt;/p&gt;
&lt;p&gt;单一插补是标准的完全数据分析方法，要优于传统的忽略缺失值的思想，使得后续的统计分析可以在完整的数据集上进行，但单一插补的插补数据都是唯一的,插补结果扭曲了样本中变量的分布，容易低估估计量的方差。&lt;/p&gt;
&lt;h4 id=&quot;多重插补&quot;&gt;&lt;a href=&quot;#多重插补&quot; class=&quot;headerlink&quot; title=&quot;多重插补&quot;&gt;&lt;/a&gt;多重插补&lt;/h4&gt;&lt;p&gt;与单一插补方法相比较，多重插补方法充分地考虑了数据的不确定性。多重插补的主要分为三个步骤，综合起来即为：插补、分析、合并。插补步是为每个缺失值都构造出 m 个可能的插补值，缺失模型具有不确定性，这些插补值能体现出模型的这个性质，利用这些可能插补值对缺失值进行插补就得到了 m 个完整数据集。分析步是对插补后的 m 个完整数据集使用一样的统计数据分析方法进行分析，同时得到 m 个统计结果。综合步就是把得到的这 m 个统计结果综合起来得到的分析结果，把这个分析结果作为缺失值的替代值。多重插补构造多个插补值主要是通过模拟的方式对估计量的分布进行推测，然后采用不同的模型对缺失值进行插补，这种插补是随机抽取的方式，这样以来能提高估计的有效性和可靠性。&lt;/p&gt;
&lt;p&gt;多重插补法主要有以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PMM法(Predictive Mean Matching,PMM)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PMM法也称随机回归插补法,它是回归插补法的变形,插补值是由回归模型的的预测值加上一个随机产生的误差项结合而成.一般而言,对应于某种确定性的插补方法,可以产生出相应的随机性插补方法.它的优点是能够在正态性假设不成立的情况下插补适当的值,缺点是随机误差项的确定比较困难.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;趋势得分法(Propensity    Score, PS)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;趋势得分是在给定观测协变量时分配给一个特殊处理的条件概率，它对每个有缺失值的变量产生一个趋势得分来表示观测缺失的概率。之后,根据这些趋势得分,把观测分组,最后再对每一组数据应用近似贝叶斯Bootstrap插补。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;马尔科夫链蒙特卡罗法(Markov Chain Monte Carlo,MCMC)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MCMC方法是蒙特卡洛重要抽样, 是流行的贝叶斯分析方法中计算后验期望的方法。一般地,MCMC将缺失数据视为参数,然后将这些参数纳入抽样的范围即可。MCMC方法对缺失数据的处理相对灵活、简单易行。&lt;/p&gt;
&lt;p&gt;多重插补方法有着其他插补方法所不具备的独特优势，第一，与其他方法相比较，多重插补方法能够尽可能的利用其他辅助信息，给出多个替代值，保持了估计结果的不确定性的大量信息；第二，多重插补方法能够尽可能接近真实情况下去模拟缺失数据的分布，在这样的条件下能够尽可能的保持变量之间的原始关系；第三，在多重插补的插补步骤会产生多个插补值，这样以来就可以利用这些不同的替代值来进一步反映无回答的不确定性。&lt;/p&gt;
&lt;h4 id=&quot;几何插补&quot;&gt;&lt;a href=&quot;#几何插补&quot; class=&quot;headerlink&quot; title=&quot;几何插补&quot;&gt;&lt;/a&gt;几何插补&lt;/h4&gt;&lt;p&gt;几何插补就是从几何的观点来研究插补，在可忽略的缺失机制下，假定观测数据的主成分与完全数据的主成分相同，那么可以通过寻找观测数据的主成分进行插补，换句话说就是用逼近观测数据的方法去插补缺失数据。&lt;/p&gt;
&lt;h3 id=&quot;基于模型的方法&quot;&gt;&lt;a href=&quot;#基于模型的方法&quot; class=&quot;headerlink&quot; title=&quot;基于模型的方法&quot;&gt;&lt;/a&gt;基于模型的方法&lt;/h3&gt;&lt;p&gt;这类方法适用于大多数场合。一般对观测数据定义一个模型，然后在模型下根据适当的分布做推断。这个方法的优势就是灵活：回避特殊情况的方法，在模型假定基础上产生的方法可以进行推演和评价；以及考虑数据不完整性时方差分析的可用性。&lt;/p&gt;
&lt;p&gt;基于模型的方法既不是删除缺失值也不是采用插补方法去补全缺失值，而是首先要考虑缺失数据的缺失机制，在此基础上为部分缺失数据定义模型，模型的参数可以通过极大似然或极大后验估计，以完全信息极大似然估计为例，这是基于模型的方法，可直接用于不完全数据的分析的，最大的特点在于即使缺失数据不是完全随机缺失，估计的结果也是无偏的。完全信息极大似然估计是建立在对数极大似然估计基础上的，假定数据来源于多元正态分布，对于不完全服从多元正态的数据还是稳健的极大似然估计的不足之处在于需要相对比较大的数据集，而且可供推断的信息是有限的。当样本量太小时，不宜采用完全信息极大似然估计。&lt;/p&gt;
&lt;h3 id=&quot;不对缺失值进行处理&quot;&gt;&lt;a href=&quot;#不对缺失值进行处理&quot; class=&quot;headerlink&quot; title=&quot;不对缺失值进行处理&quot;&gt;&lt;/a&gt;不对缺失值进行处理&lt;/h3&gt;&lt;p&gt;直接在包含空值的数据上进行数据挖掘。可通过贝叶斯网络和人工神经网络实现。&lt;/p&gt;
&lt;p&gt;贝叶斯网络是用来表示变量间连接概率的图形模式，它提供了一种自然的表示因果信息的方法，用来发现数据间的潜在关系。在这个网络中，用节点表示变量，有向边表示变量间的依赖关系。贝叶斯网络仅适合于对领域知识具有一定了解的情况，至少对变量间的依赖关系较清楚的情况。否则直接从数据中学习贝叶斯网的结构不但复杂性较高（随着变量的增加，指数级增加），网络维护代价昂贵，而且它的估计参数较多，为系统带来了高方差，影响了它的预测精度。当在任何一个对象中的缺失值数量很大时，存在指数爆炸的危险。&lt;/p&gt;
&lt;p&gt;基于神经网络进行缺失数据估计的基本思想是：把同一系统的除待估计参数以外的其它参数的数据作为网络的输入，待估计参数的数据作为输出，利用该系统中的已知数据训练网络，在网络满足要求后，把与待估计的数据同一时间的其它参数的数据输入网络，网络输出值即为缺失数据的估计值。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;数据缺失的概念&quot;&gt;&lt;a href=&quot;#数据缺失的概念&quot; class=&quot;headerlink&quot; title=&quot;数据缺失的概念&quot;&gt;&lt;/a&gt;数据缺失的概念&lt;/h2&gt;&lt;p&gt;数据缺失是指在数据采集时由于某种原因应该得到而没有得到的数据。它指的是现有数据集中某个或某些属性的值
    
    </summary>
    
      <category term="机器学习" scheme="http://yunxileo.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数据预处理" scheme="http://yunxileo.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
    
      <category term="缺失值处理" scheme="http://yunxileo.github.io/tags/%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>logistic_regression</title>
    <link href="http://yunxileo.github.io/2016/05/16/logistic-regression/"/>
    <id>http://yunxileo.github.io/2016/05/16/logistic-regression/</id>
    <published>2016-05-16T15:57:47.000Z</published>
    <updated>2016-05-19T06:21:53.546Z</updated>
    
    <content type="html">&lt;h2 id=&quot;引子&quot;&gt;&lt;a href=&quot;#引子&quot; class=&quot;headerlink&quot; title=&quot;引子&quot;&gt;&lt;/a&gt;引子&lt;/h2&gt;&lt;p&gt;大家在日常的工作和学习中是不是经常有这样的疑问：邮箱是如何自动区分正常邮件和垃圾邮件的呢？银行是如何判断是否通过你的贷款申请的呢？经常收到某种商品的推荐信息，商家又是如何知道你对这个商品感兴趣的呢？&lt;/p&gt;
&lt;p&gt;为了回答上述疑问，这一期给大家介绍逻辑回归算法。逻辑回归，也称Logistic Regression,主要区别于一般的线性回归模型。我们知道，一般的线性回归模型都是处理因变量是连续变量的问题，如果因变量是分类变量，一般线性回归模型就不再适用。逻辑回归算法因其原理的相对简单，可解释性强等优点已成为互联网领域最常用也最有影响力的分类算法之一，同时它还可以作为众多集成算法以及深度学习的基本组成单位，所以学好逻辑回归尤其重要。&lt;/p&gt;
&lt;p&gt;我们知道，一般的线性回归模型都是处理因变量是连续变量的问题，如果因变量是定性变量，一般线性回归模型就不再适用了。&lt;/p&gt;
&lt;p&gt;或许有人会有疑问，为什么对于分类问题，逻辑回归行而一般的线性回归模型却不行呢？二者的区别又是什么呢？下面将从现实意义和数学理论上给出解释。&lt;/p&gt;
&lt;h3 id=&quot;定性因变量回归方程的意义&quot;&gt;&lt;a href=&quot;#定性因变量回归方程的意义&quot; class=&quot;headerlink&quot; title=&quot;定性因变量回归方程的意义&quot;&gt;&lt;/a&gt;定性因变量回归方程的意义&lt;/h3&gt;&lt;p&gt;设因变量y是只取0,1两个值，考虑简单线性回归模 $y=\beta_{0}+\beta_{1}x_{i} +\varepsilon$ &lt;br&gt;在这种y只取0和1两个值的情况下，因变量均值  $E(y_{i})=\beta_{0}+\beta_{1}x_{i}$  有着特殊的意义。&lt;br&gt;由于$y$是0-1型随机变量，得到如下概率分布&lt;br&gt; $$ P(y = 1)= p$$&lt;br&gt; $$ P(y = 0)= 1-p $$ &lt;br&gt;根据离散型随机变量期望值的定义，可得&lt;br&gt;$$ E(y)=1( p )+0(1-p)= p $$&lt;br&gt;所以，作为由回归函数给定的因变量均值，$E(y)=\beta_{0}+\beta_{1}x$是自变量水平为$x$时$y=1$的概率。&lt;/p&gt;
&lt;h3 id=&quot;逻辑回归模型的特别之处&quot;&gt;&lt;a href=&quot;#逻辑回归模型的特别之处&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归模型的特别之处&quot;&gt;&lt;/a&gt;逻辑回归模型的特别之处&lt;/h3&gt;&lt;p&gt;对于一般的线性模型&lt;br&gt;
$$y=\beta_{0}+\beta_{1}x +\varepsilon$$
&lt;br&gt;误差项有大三假定条件：&lt;/p&gt;
&lt;p&gt;（1）误差项$\varepsilon$是一个期望为0的随机变量，即$E(\varepsilon)=0$&lt;/p&gt;
&lt;p&gt;（2）对于所有的$x$，$\varepsilon$的方差都相同，这意味着对于一个特定的x值，y的方差也都等于$\sigma^2$。&lt;/p&gt;
&lt;p&gt;（3）误差项$\varepsilon$是一个服从正态分布的随机变量，且相互独立，即$\varepsilon\sim N(0,\sigma^2)$。&lt;/p&gt;
&lt;p&gt;而在因变量y只能取0和1的逻辑回归模型，误差项  $\varepsilon=y-(\beta_{0}+\beta_{1}x)$ 显然是两点型的离散分布，不满足误差项正态分布的基本假定；同时误差项的方差 $Var(\varepsilon_{i})=Var(y_{i})=(\beta_{0}+\beta_{1}x_{i})(1-\beta_{0}-\beta_{1}x_{i})$,可以看出误差项随着$x$的不同水平而变化，是异方差，不满足线性回归的基本假定；当因变量为0和1时，回归方程代表的是概率分布，所以因变量的均值受到如下限制$0\leq E(y_{i})\leq1$,一般的线性回归方程不会有这种限制。而逻辑回归却利用一些数学变化巧妙的解决了这些的问题，请看下面一节。&lt;/p&gt;
&lt;h3 id=&quot;从一般线性回归到逻辑回归&quot;&gt;&lt;a href=&quot;#从一般线性回归到逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;从一般线性回归到逻辑回归&quot;&gt;&lt;/a&gt;从一般线性回归到逻辑回归&lt;/h3&gt;&lt;p&gt;当被解释变量$y$为0和1的二分类变量时，虽然无法采用一般的线性回归模型建模，但是可以借鉴其理论基础：&lt;/p&gt;
&lt;p&gt;第一，一般线性模型$p(y=1|x)=\beta_{0}+\beta_{1}x_{1}+\dots+\beta_{p}x_{p}$，方程左侧的概率$p$的取值范围为[0,1]，方程右边的额取值范围在$-\infty\sim +\infty$之间。如果对概率p做合理的变换，使其的取值范围与右侧吻合，则左侧和右侧可以通过等号连接起来。&lt;/p&gt;
&lt;p&gt;第二，一般线性模型$p(y=1|x)=\beta_{0}+\beta_{1}x_{1}+\dots+\beta_{p}x_{p}$，方程中的概率$p$与解释变量之间的关系是线性的。但在实际的应用中，它们之间的关系往往是非线性的。例如通过银行贷款申请的概率通常不会随着年收入（或者年龄等）的增长而线性增长。于是对概率$p$的变换应该是采用非线性变换。&lt;/p&gt;
&lt;p&gt;基于以上的分析，可采取一下两步变换：&lt;/p&gt;
&lt;p&gt;第一步，将概率$p$转换成$\Omega:\Omega=\frac{p}{1-p}$。其中，$\Omega$称为优势，是事件发生的概率与不发生的概率之比。这种变换是非线性的，且$\Omega$是$p$的单调函数，保证了$\Omega$和$p$增长的一致性，是模型易于理解。优势的取值范围在0和无穷大之间。&lt;/p&gt;
&lt;p&gt;第二步，将$\Omega$换成$ln\Omega:ln\Omega=ln\frac{p}{1-p}$。其中，$ln\Omega$称为$logit P$. &lt;/p&gt;
&lt;p&gt;上述的两步变换称为logit变换。经过logit变换，$logit P$的取值范围范围$-\infty\sim+\infty$，与一般线性回归模型右侧的取值范围吻合。同时$logitP$与$p$之间保持单调一致性。&lt;/p&gt;
&lt;p&gt;至此，用等号将$logitP$和一般线性模型的右侧连接起来，得到$logit P=\beta_{0}+\beta_{1}x_{1}+\dots+\beta_{p}x_{p}$,即为逻辑回归模型。这样我们就完成从一般线性模型到逻辑回归模型的演变。&lt;/p&gt;
&lt;p&gt;或许有人还会质疑logit变换的合理性，那么我们就继续往下扒。&lt;br&gt;从以上的推导和变换我们得到，$$ln\frac{p}{1-p}=\beta_{0}+\beta_1 x_{1}+\beta_2 x_{i}+\cdots +\beta_p x_{p}$$&lt;br&gt;故有 $p=\frac{1}{1+e^{-(\beta_{0}+\beta_1 x_{1}+\beta_2 x_{2}+\cdots +\beta_p x_{p})}}$，其为(0,1)型的Sigmoid函数，如下图所示。这是一个非线性函数，很好的体现了概率$p$与解释变量之间的非线性关系。&lt;/p&gt;
&lt;h2 id=&quot;逻辑回归模型的解读&quot;&gt;&lt;a href=&quot;#逻辑回归模型的解读&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归模型的解读&quot;&gt;&lt;/a&gt;逻辑回归模型的解读&lt;/h2&gt;&lt;p&gt;逻辑回归方程的右侧与一般线性回归方程的形式一致，可用类似的方法解释逻辑回归方程系数的含义，即当其他自变量保持不变时，自变量$x_i$每增加一个单位，$logitP$平均增加（或减少）$\beta_{i}$个单位。&lt;/p&gt;
&lt;p&gt;在实际应用中，人们更关心自变量为优势$\Omega$带来的变化，其中优势$\Omega=\frac{p}{1-p}$，表示某一事件的发生概率与不发生概率之比。同时我们还会通过优势比来进行不同组别之间风险的对比分析。&lt;/p&gt;
&lt;p&gt;在逻辑回归方程中，$\Omega=e^{(\beta_{0}+\beta_1 x_{1}+\beta_2 x_{2}+\cdots +\beta_p x_{p})}$，当其他自变量不变时，$x_{i}$每增加一个单位，优势变为原来优势的$e^{\beta_i}$倍，优势比即为$e^{\beta_i}$。&lt;/p&gt;
&lt;h2 id=&quot;逻辑回归模型的参数估计&quot;&gt;&lt;a href=&quot;#逻辑回归模型的参数估计&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归模型的参数估计&quot;&gt;&lt;/a&gt;逻辑回归模型的参数估计&lt;/h2&gt;&lt;p&gt;设y是0-1型变量，$x_{1},x_{2},\cdots ,x_{p}$是与y相关的确定性变量，n组观测数据为$(x_{i1},x_{i2},\cdots ,x_{ip};y_{i})(i=1,2,\cdots,n)$,其中，$y_{1},y_{2},\cdots ,y_{n}$是取值0或1的随机变量，$y_{i}$与$x_{i1},x_{i2},\cdots ,x_{ip}$的关系如下：&lt;br&gt; 
$$E(y_{i})=p_{i}=f(\beta_{0}+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots +\beta_p x_{ip})$$
 &lt;br&gt;其中，函数f(x)是值域在[0,1]区间内的单调增函数。对于逻辑回归&lt;br&gt; 
$$f(x)=\frac{e^{x}}{1+e^{x}}$$
 &lt;br&gt;于是$y_{i}$是均值为$p_{i}=f(\beta_{0}+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots +\beta_p x_{ip})$的0-1分布，概率函数为&lt;br&gt; 
$$P(y_{i}=1)=p_{i}$$
$$P(y_{i}=0)=1-p_{i}$$
&lt;br&gt;可以把$y_{i}$的概率函数合写为&lt;br&gt; 
$$P(y_{i})=p_{i}^{y_{i}}(1-p_{i})^{1-y_{i}},y_{i}=0,1;i=1,2,\cdots,n$$
 &lt;br&gt;于是，$y_{1},y_{2},\cdots ,y_{n}$的似然函数为&lt;br&gt; 
$$L=\prod_{i=1}^{n}P(y_{i})=\prod_{i=1}^{n}p_{i}^{y_{i}}(1-p_i)^{1-y_i}$$
 &lt;br&gt;对似然函数取自然对数，得&lt;br&gt; 
$$lnL=\sum_{i=1}^{n}[y_i lnp_i+(1-y_i)ln(1-p_i)]\\
   =\sum_{i=1}^{n}[y_i ln\frac{p_i}{(1-p_i)}+ln(1-p_i)]$$
  &lt;br&gt;对于logistic回归，将&lt;br&gt; 
$$p_i=\frac{e^{(\beta_{0}+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots +\beta_p x_{ip})}}{1+e^{(\beta_{0}+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots +\beta_p x_{ip})}}$$
 &lt;br&gt;代入得&lt;br&gt; 
$$lnL=\sum_{i=1}^{n}[y_i (\beta {0}+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots +\beta_p x_{ip})-ln(1+e^{(\beta_{0}+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots +\beta_p x_{ip})})]$$
 &lt;br&gt;最大似然估计就是选取$\beta_{0},\beta_1,\beta_2,\cdots ,\beta_p$的估计值$\hat{\beta}_0,\hat{\beta}_1,\hat{\beta}_2,\cdots,\hat{\beta}_p$，使上式最大。同时，作为一个最优化问题，可以采用梯度下降法和牛顿法等最优化算法。&lt;/p&gt;
&lt;h2 id=&quot;逻辑回归模型的检验&quot;&gt;&lt;a href=&quot;#逻辑回归模型的检验&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归模型的检验&quot;&gt;&lt;/a&gt;逻辑回归模型的检验&lt;/h2&gt;&lt;p&gt;逻辑回归方程的显著性检验的目的是检验所有自变量与$logitP$的线性关系是否显著，是否可以选择线性模型。原假设是假设各回归系数同时为0，自变量全体与$logitP$的线性关系不显著。如果方程中的诸多自变量对$logitP$的线性解释有显著意义，那么必然会使回归方程对样本的拟合得到显著提高。可通过对数似然比测度拟合程度是否有所提高。&lt;/p&gt;
&lt;p&gt;我们通常采用似然比检验统计量$-ln(\frac{L}{L_{x_{i}}})^2$也可称为似然比卡方,其中$L$表示引入变量$x_{i}$前回归方程的似然函数值，$L_{x_{i}}$表示引入变量$x_{i}$后回归方程的似然函数值。似然比检验统计量越大表明引入变量$x_{i}$越有意义。如果似然比卡方观测值的概率p值小于给定的显著性水平，不接受原假设，即认为自变量全体与$logitP$之间的线性关系显著。反之，线性关系不显著。&lt;/p&gt;
&lt;h2 id=&quot;回归系数的显著性检验&quot;&gt;&lt;a href=&quot;#回归系数的显著性检验&quot; class=&quot;headerlink&quot; title=&quot;回归系数的显著性检验&quot;&gt;&lt;/a&gt;回归系数的显著性检验&lt;/h2&gt;&lt;p&gt;逻辑回归系数的显著性检验是检验方程中各变量与$logitP$之间是否具有线性关系。原假设是假设变量$x_{i}$与$logitP$之间的线性关系不显著，即$\beta_{i}=0$。&lt;/p&gt;
&lt;h2 id=&quot;后记&quot;&gt;&lt;a href=&quot;#后记&quot; class=&quot;headerlink&quot; title=&quot;后记&quot;&gt;&lt;/a&gt;后记&lt;/h2&gt;&lt;p&gt;逻辑回归虽然虽然简单，但是因为其运算过程简单，而且分类效果不会太差，所以在业界应用广泛。我们大名鼎鼎的围棋高手AlphaGo在快速走子的过程中，也有用到该算法哟。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引子&quot;&gt;&lt;a href=&quot;#引子&quot; class=&quot;headerlink&quot; title=&quot;引子&quot;&gt;&lt;/a&gt;引子&lt;/h2&gt;&lt;p&gt;大家在日常的工作和学习中是不是经常有这样的疑问：邮箱是如何自动区分正常邮件和垃圾邮件的呢？银行是如何判断是否通过你的贷款申请的呢？经常收到某
    
    </summary>
    
      <category term="机器学习" scheme="http://yunxileo.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://yunxileo.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
    
      <category term="逻辑回归" scheme="http://yunxileo.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
